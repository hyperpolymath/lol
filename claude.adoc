= 1000Langs Project Guide

== Project Overview

1000Langs is a super-parallel corpus crawler designed for multilingual NLP and Computational Linguistics research. The project crawls 1500+ unique languages from multiple Bible corpora sources to build a massive parallel text collection.

* Language: Python 3.6+
* License: Apache 2.0
* Maintainer: Ehsaneddin Asgari (LMU, Munich)

== Project Structure

----
1000Langs/
├── biblecrawler/           # Core web crawlers
│   ├── general_crawler.py  # Base crawler with HTTP session management
│   ├── general_parser.py   # Base parser for verse extraction
│   ├── bibleCLOUD.py       # bible.cloud crawler
│   ├── bibleCOM.py         # bible.com crawler
│   ├── bibleIS.py          # Icelandic Bible sources
│   └── biblePngscripturesORG.py  # pngscriptures.org crawler
├── bdpAPI/                 # Bible Digital Platform API wrapper
├── bibleCLOUDAPI/          # Bible Cloud API wrapper
├── bibleCOMAPI/            # Bible.com API wrapper
├── biblePNGAPI/            # PNG Scripture API wrapper
├── metaAPI/                # Metadata retrieval
├── massive_parallelbible_IF/  # Data access interface
├── utility/                # Helper utilities
│   ├── file_utility.py     # File I/O, pickling
│   ├── math_utility.py     # KL-divergence, clustering
│   ├── visualization_utility.py
│   ├── labeling_utility.py
│   ├── list_set_util.py    # Set operations, n-grams
│   └── interface_util.py   # User prompts
├── wals/                   # Language features (WALS integration)
├── run_crawler/            # Main entry point
│   └── lang1000.py         # CLI orchestration
└── meta/                   # Reference data files
----

== Key Dependencies

* Web Scraping: beautifulsoup4, lxml
* HTTP: requests
* Data Processing: pandas, numpy
* Science: scikit-learn, scipy
* Visualization: matplotlib, seaborn

== Development Guidelines

=== Code Style
* Follow PEP 8 conventions
* Use descriptive variable names
* Document public functions with docstrings

=== Error Handling
* Crawlers should handle network errors gracefully
* Log errors rather than silently failing
* Validate API responses before processing

== Testing Requirements

=== Current State
The project currently has NO automated tests. This is a critical gap that needs to be addressed.

=== Recommended Test Structure

----
tests/
├── conftest.py                 # pytest fixtures
├── unit/
│   ├── test_file_utility.py
│   ├── test_math_utility.py
│   ├── test_list_set_util.py
│   └── test_metadata.py
├── integration/
│   ├── test_api_bdp.py
│   ├── test_api_biblecloud.py
│   ├── test_api_biblecom.py
│   └── test_accessbible.py
├── crawlers/
│   ├── test_general_crawler.py
│   ├── test_general_parser.py
│   └── test_*_crawler.py
└── fixtures/
    ├── sample_html/
    └── mock_responses.py
----

=== Testing Priorities

==== High Priority
1. **API Integration Tests** - Mock responses for each API wrapper
2. **Web Crawler Tests** - Mock HTTP responses, test parsing logic
3. **Data Parser Tests** - Bible verse extraction with various HTML
4. **File I/O Tests** - Pickle serialization, directory operations
5. **Data Access Layer** - Corpus retrieval with sample data

==== Medium Priority
1. **Metadata Processing** - BeautifulSoup parsing, pandas operations
2. **Mathematical Functions** - KL-divergence, clustering algorithms
3. **Utility Functions** - Sampling, n-grams, set operations

==== Low Priority
1. **CLI Argument Parsing** - Command-line interface tests
2. **User Prompts** - Interactive input handling
3. **Visualization** - Chart generation tests

=== Critical Testing Gaps

* No error handling tests for network failures
* No retry logic validation
* No data integrity verification
* No concurrency/race condition tests
* No regression tests for website structure changes
* No performance benchmarks

=== Recommended Testing Tools

* pytest - Test framework
* pytest-cov - Coverage reporting
* responses or requests-mock - HTTP mocking
* pytest-mock - General mocking
* hypothesis - Property-based testing for utilities

== Running the Project

[source,bash]
----
# Install dependencies
pip install -r requirement.txt

# Run the crawler
python run_crawler/lang1000.py
----

== API Keys

Some API wrappers require authentication. Check individual API modules for configuration requirements.
